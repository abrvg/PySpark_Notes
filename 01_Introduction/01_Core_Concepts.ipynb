{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Concepts\n",
    "\n",
    "## Spark Architecture\n",
    "Apache Spark is an open-source, framework-based, developed to process large amount of data, structured or unstructured using cluster computation.\n",
    "The main advantage of Spark is to use the in-memory cluster computing, implicit data parallelism and fault tolerance. It is 100x faster than MapReduce, powerful for caching and disk persistence.\n",
    "\n",
    "![some text](../images/Intro_Spark_Arch.png)\n",
    "\n",
    "### Driver\n",
    "It is responsible for instantiationg the Spark Driver and Session. It communicates with the cluster manager and requests for resources (memory, cpu, Executors). Also, it transforms the Spark operations into DAGs and schedule them.\n",
    "\n",
    "### Cluster Manager\n",
    "It is responsible for managing and allocate resources for the nodes.\n",
    "\n",
    "### Executor\n",
    "Runs on each worker node and also communicates with the driver, It executes the distributed tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets\n",
    "RDD (Resilient Distributed Dataset): Think of an RDD as a collection of data items split across different computers. Spark can perform calculations on these items in parallel, making it super fast for processing large amounts of data.\n",
    "\n",
    "Shared Variables: When Spark does calculations, it usually sends a copy of the data it needs to each computer. But sometimes, we want to share data between all the computers or collect results from all of them. This is where shared variables come in.\n",
    "\n",
    "- Broadcast Variables: These are like read-only copies of data that Spark sends to every computer once. This is useful for sharing large datasets that are used in many calculations.\n",
    "\n",
    "- Accumulators: These are special variables that can only be added to. They're used to collect information from all the computers, like counting things or summing numbers.\n",
    "In essence, RDDs are for distributing data, and shared variables help coordinate calculations across different computers in a Spark cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation\n",
    "Parallelized collections are created by calling SparkContextâ€™s parallelize method on an existing iterable or collection in your driver program.\n",
    "\n",
    "One important parameter for parallel collections is the number of partitions to cut the dataset into. Spark will run one task for each partition of the cluster. Typically you want 2-4 partitions for each CPU in your cluster. Normally, Spark tries to set the number of partitions automatically based on your cluster. However, you can also set it manually by passing it as a second parameter to parallelize (e.g. sc.parallelize(data, 10)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", appName=\"RDD_Example\")\n",
    "\n",
    "# Create a list of numbers\n",
    "data = range(40)\n",
    "\n",
    "# Create an RDD from the list, set the number of partitions\n",
    "num_partitions = 4\n",
    "rdd = sc.parallelize(data, num_partitions)\n",
    "\n",
    "# Show the data in each partition\n",
    "for i in range(num_partitions):\n",
    "    print(f\"Data in Partition {i} -> {rdd.glom().collect()[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "Transformations in Spark are operations that create a new RDD from an existing one. Importantly, these operations are lazy, meaning they are not executed immediately. Instead, they are remembered and only executed when an action is triggered. This allows Spark to optimize the execution plan and improve performance.\n",
    "\n",
    "Some common transformations: map, filter, flatMap, groupBy, join, union, distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example filter the odd numbers from the previous rdd\n",
    "# The operation is not going to be performed until an action is called\n",
    "rdd_2 = rdd.filter(lambda x: x % 2 != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions\n",
    "Actions in Spark trigger the execution of transformations and return a result to the driver program.\n",
    "\n",
    "Some common actions: collect(), count(), first(), take(n), reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the action of the rdd_2 (odd numbers filtered) \n",
    "# Notice the  number of partitions are inherited\n",
    "for i in range(num_partitions):\n",
    "    print(f\"Data in Partition {i} -> {rdd_2.glom().collect()[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish the spark Context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrame Transformation\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame\n",
    "data = [(\"Alice\", 30), (\"Bob\", 35), (\"Charlie\", 40)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the original DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only name column\n",
    "#df.select(\"Name\").show()\n",
    "\n",
    "# Select people older than 32\n",
    "df.filter(df['Age'] > 32).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sql function on a SparkSession enables applications to run SQL queries programmatically and returns the result as a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Use SQL syntax \n",
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finish the spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "\n",
    "Data Ingestion: Learn to read data from various sources (CSV, JSON, Parquet, databases, etc.) into PySpark DataFrames.\n",
    "\n",
    "Data Cleaning and Transformation: Master data cleaning techniques, handling missing values, outliers, and data transformations using PySpark functions.\n",
    "\n",
    "Data Aggregation and Analysis: Perform aggregations, grouping, and statistical analysis on large datasets using PySpark.\n",
    "\n",
    "Machine Learning with PySpark: Explore MLlib for basic machine learning tasks.\n",
    "\n",
    "Performance Optimization: Learn techniques to optimize PySpark code for performance, including caching, partitioning, and broadcasting.\n",
    "\n",
    "Distributed Computing Concepts: Understand concepts like partitioning, shuffling, and task scheduling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
