{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction: Let's generate some random and noisy data about a table of a car sales company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "def generate_car_data(n_rows):\n",
    "    \"\"\"Generates a CSV file with random car sales data.\n",
    "\n",
    "    Args:\n",
    "        n_rows: Number of rows to generate.\n",
    "    \"\"\"\n",
    "\n",
    "    with open('car_sales.csv', 'w', newline='') as csvfile:\n",
    "        fieldnames = ['id', 'model', 'year', 'mileage', 'price', 'color']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "\n",
    "        for i in range(n_rows):\n",
    "            row = {}\n",
    "            row['id'] = i + 1\n",
    "            row['model'] = random.choice(['Camry', 'Civic', 'F-150', 'Silverado', 'Altima', 'Sierra', 'Model Y', 'RAV4', 'Wrangler'])\n",
    "            row['year'] = random.randint(2010, 2024) + np.random.choice([-10, 0, 10], p=[0.05, 0.85, 0.1])  # Introduce outliers in year\n",
    "            row['mileage'] = random.randint(10000, 100000) + np.random.choice([-50000, 0, 50000], p=[0.05, 0.85, 0.1])  # Introduce outliers in mileage\n",
    "            row['price'] = random.randint(10000, 50000)\n",
    "            row['price'] = None if random.random() < 0.05 else row['price']  # Introduce null values in price\n",
    "            row['mileage'] = None if random.random() < 0.05 else row['mileage']  # Introduce null values in mileage\n",
    "            row['color'] = random.choice(['Red', 'Blue', 'White', 'Black', 'Silver'])\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 1000\n",
    "generate_car_data(n_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the spark session and load the data previuosly created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/27 18:05:28 WARN Utils: Your hostname, air.local resolves to a loopback address: 127.0.0.1; using 192.168.5.143 instead (on interface en0)\n",
      "24/08/27 18:05:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/27 18:05:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataCleaning\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"car_sales.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the number of missing values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values: {'id': 0, 'model': 0, 'year': 0, 'mileage': 52, 'price': 40, 'color': 0}\n"
     ]
    }
   ],
   "source": [
    "missing_values_dict = {col: df.filter(df[col].isNull()).count() for col in df.columns}\n",
    "print(\"Number of missing values:\", missing_values_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the use case we have different strategies to follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912\n"
     ]
    }
   ],
   "source": [
    "df_dropped = df.dropna()\n",
    "print(df_dropped.count()) # Count the final number of rows after removing the nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute the mileage with zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of missing values in mileage column 0\n"
     ]
    }
   ],
   "source": [
    "df_zero_imp = df.fillna(0)\n",
    "\n",
    "print(\"number of missing values in mileage column\", df_zero_imp.filter(df_zero_imp['mileage'].isNull()).count() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean imputation. \n",
    "Let's impute the price with the mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(inputCols=[\"price\"], outputCols=[\"imputed_price\"], strategy=\"mean\")\n",
    "df_mean_imputed = imputer.fit(df).transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values after mean imputation: {'id': 0, 'model': 0, 'year': 0, 'mileage': 52, 'price': 40, 'color': 0, 'imputed_price': 0}\n"
     ]
    }
   ],
   "source": [
    "missing_values_mean_imp = {col: df_mean_imputed.filter(df_mean_imputed[col].isNull()).count() for col in df_mean_imputed.columns}\n",
    "print(\"Number of missing values after mean imputation:\", missing_values_mean_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode imputation. \n",
    "Let's impute the mileage with the mode (no reason to do it other than the exercise purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values after mode imputation: {'id': 0, 'model': 0, 'year': 0, 'mileage': 52, 'price': 40, 'color': 0, 'imputed_mileage': 0}\n"
     ]
    }
   ],
   "source": [
    "imputer = Imputer(inputCols=[\"mileage\"], outputCols=[\"imputed_mileage\"], strategy=\"mode\")\n",
    "df_mode_imputed = imputer.fit(df).transform(df)\n",
    "\n",
    "missing_values_mode_imp = {col: df_mode_imputed.filter(df_mode_imputed[col].isNull()).count() for col in df_mode_imputed.columns}\n",
    "print(\"Number of missing values after mode imputation:\", missing_values_mode_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other method can be interpolation. Or impute based on grouping conditions, we will review this later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Outliers: Z Score Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'((price - avg(price)) / stddev_pop(price))'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import stddev_pop, mean\n",
    "\n",
    "z_score = (col(\"price\") - mean(col(\"price\"))) / stddev_pop(col(\"price\"))\n",
    "print(z_score)\n",
    "#df_outliers_z = df.filter(abs(z_score) > 3)\n",
    "\n",
    "#df_cleaned_z = df.subtract(df_outliers_z)\n",
    "\n",
    "#print(\"number of rows after z score removal\", df_cleaned_z.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
